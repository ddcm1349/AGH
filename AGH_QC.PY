#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AGH è§„åˆ™åˆå¹¶å»é‡å·¥å…· | é«˜æ€§èƒ½å¼‚æ­¥é€»è¾‘ç‰ˆ
ä¼˜åŒ–ï¼šå¼•å…¥å±‚çº§å“ˆå¸Œç®—æ³•ï¼Œå¤„ç†æ•°ä¸‡æ¡è§„åˆ™ä»…éœ€ç§’çº§
"""

import requests
import time
from datetime import datetime, timedelta
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# --- é…ç½®åŒº ---
AGH_RULE_URLS = [
    "https://adrules.top/dns.txt",
    "https://anti-ad.net/easylist.txt",
    "https://raw.githubusercontent.com/blackmatrix7/ios_rule_script/master/rule/AdGuard/Privacy/Privacy.txt",
]
AGH_RULE_NAMES = ["AdRules", "anti-ad", "Privacy"]
AGH_OUTPUT_FILE = "adguard_rules_qx.txt"
SUBSCRIBE_URL = "https://ddcm1349.github.io/AGH/adguard_rules_qx.txt"
TIMEOUT = 30

def create_retry_session():
    """åˆ›å»ºå¸¦é‡è¯•æœºåˆ¶çš„è¯·æ±‚ä¼šè¯"""
    session = requests.Session()
    retry = Retry(
        total=3,
        backoff_factor=1,
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods=["GET"]
    )
    session.mount("https://", HTTPAdapter(max_retries=retry))
    session.mount("http://", HTTPAdapter(max_retries=retry))
    return session

def fetch_remote_content(url, source_name):
    """æ‹‰å–è¿œç¨‹è§„åˆ™"""
    try:
        print(f"ğŸ“¥ æ­£åœ¨æ‹‰å–ã€{source_name}ã€‘...")
        headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
        resp = create_retry_session().get(url, headers=headers, timeout=TIMEOUT)
        resp.raise_for_status()
        lines = resp.text.splitlines()
        print(f"âœ… æˆåŠŸ | åŸå§‹è¡Œæ•°: {len(lines)}")
        return lines
    except Exception as e:
        print(f"âŒ å¤±è´¥ | {source_name}: {str(e)}")
        return []

def count_rule_types(rule_list):
    """ç»Ÿè®¡é»‘ç™½åå•æ•°é‡"""
    black = sum(1 for r in rule_list if r.startswith("||"))
    white = sum(1 for r in rule_list if r.startswith("@@"))
    other = len(rule_list) - black - white
    return black, white, other

def apply_containment_dedup(rules, rule_type="è§„åˆ™"):
    """
    âš¡ é«˜æ€§èƒ½åŒ…å«å»é‡ç®—æ³•
    é€»è¾‘ï¼šæŒ‰åŸŸåå±‚çº§æ’åºï¼Œåˆ©ç”¨ set è®°å½•çˆ¶åŸŸï¼Œå®ç°å­åŸŸç§’çº§å‰ªæ
    """
    if not rules:
        return [], 0
    
    start_time = time.time()
    # é¢„å¤„ç†ï¼šæå–åŸŸåå…³é”®éƒ¨åˆ†å¹¶è®°å½•åŸå§‹è§„åˆ™
    processed = []
    for r in rules:
        # ç§»é™¤ä¿®é¥°ç¬¦ï¼Œä»…æå–åŸŸåä¸»ä½“è¿›è¡Œé€»è¾‘åˆ¤æ–­
        domain = r.split('$')[0].replace('||', '').replace('@@', '').replace('^', '').lower()
        if domain:
            processed.append((domain, r))
    
    # æ ¸å¿ƒï¼šæŒ‰åŸŸåç‚¹å·æ•°é‡æ’åºï¼ˆçŸ­åŸŸå/çˆ¶åŸŸåæ’åœ¨å‰é¢ï¼‰
    processed.sort(key=lambda x: x[0].count('.'))
    
    final_rules = []
    seen_domains = set() # å“ˆå¸Œé›†åˆï¼ŒæŸ¥æ‰¾æ•ˆç‡ O(1)
    
    for dom, original in processed:
        is_subdomain = False
        parts = dom.split('.')
        # é€å±‚å‘ä¸ŠæŸ¥æ‰¾çˆ¶åŸŸåæ˜¯å¦å·²åœ¨é›†åˆä¸­
        # ä¾‹å¦‚: www.example.com ä¼šæŸ¥æ‰¾ example.com å’Œ com
        for i in range(len(parts) - 1, 0, -1):
            parent = ".".join(parts[i:])
            if parent in seen_domains:
                is_subdomain = True
                break
        
        if not is_subdomain:
            final_rules.append(original)
            seen_domains.add(dom)
    
    end_time = time.time()
    removed = len(rules) - len(final_rules)
    print(f"âœ‚ï¸ {rule_type}åŒ…å«å»é‡å®Œæˆ | è€—æ—¶: {end_time - start_time:.2f}s | å»é™¤: {removed} æ¡")
    return final_rules, removed

def main():
    total_start = time.time()
    all_raw_rules = []
    source_stats = []
    
    print("="*60 + "\nğŸš€ å¼€å¯ AGH è§„åˆ™è‡ªåŠ¨åŒ–åˆå¹¶å¤„ç†\n" + "="*60)

    # 1. åˆ†å—æ‹‰å–ä¸åˆæ­¥æ¸…æ´—
    for url, name in zip(AGH_RULE_URLS, AGH_RULE_NAMES):
        raw_lines = fetch_remote_content(url, name)
        valid_rules = [line.strip() for line in raw_lines if line.strip() and not line.startswith(('!', '#'))]
        
        # è®°å½•æºä¿¡æ¯
        b, w, o = count_rule_types(valid_rules)
        source_stats.append({
            "name": name, "total": len(valid_rules), 
            "black": b, "white": w, "other": o, "raw": len(raw_lines)
        })
        all_raw_rules.extend(valid_rules)

    # 2. å…¨å±€åŸºç¡€å»é‡ (å»é™¤å®Œå…¨ç›¸åŒçš„è¡Œ)
    unique_rules = list(dict.fromkeys(all_raw_rules))
    cross_dedup = len(all_raw_rules) - len(unique_rules)
    print(f"\nğŸ”„ å…¨å±€å»é‡: {len(all_raw_rules)} -> {len(unique_rules)} (å»é™¤é‡å¤: {cross_dedup})")

    # 3. åˆ†ç±»å­˜å‚¨
    white_list = [r for r in unique_rules if r.startswith("@@")]
    black_list = [r for r in unique_rules if r.startswith("||")]
    other_list = [r for r in unique_rules if not (r.startswith("@@") or r.startswith("||"))]

    # 4. æ‰§è¡Œé«˜æ€§èƒ½åŒ…å«å»é‡
    white_final, w_rem = apply_containment_dedup(white_list, "ç™½åå•")
    black_final, b_rem = apply_containment_dedup(black_list, "é»‘åå•")

    # 5. åˆå¹¶ç»“æœ
    final_output = white_final + black_final + other_list
    final_count = len(final_output)
    
    # 6. ç”Ÿæˆ Header å¹¶å†™å…¥æ–‡ä»¶
    beijing_time = (datetime.utcnow() + timedelta(hours=8)).strftime('%Y-%m-%d %H:%M:%S')
    
    stats_lines = [f"#   {i+1}. {s['name']} | åŸå§‹:{s['total']} (é»‘:{s['black']} ç™½:{s['white']})" for i, s in enumerate(source_stats)]
    
    header = [
        "!", f"! AGH è§„åˆ™åˆå¹¶ (é«˜æ€§èƒ½å»é‡ç‰ˆ)",
        f"! ç”Ÿæˆæ—¶é—´: {beijing_time} (åŒ—äº¬æ—¶é—´)",
        f"! æœ€ç»ˆæ€»æ•°: {final_count} æ¡",
        "! " + "="*50,
        "! å„æºç»Ÿè®¡:",
        *stats_lines,
        "! " + "="*50,
        f"! å»é‡æ±‡æ€»: åŸºç¡€å»é‡ {cross_dedup} | åŒ…å«å»é‡(é»‘) {b_rem} | åŒ…å«å»é‡(ç™½) {w_rem}",
        "! " + "="*50,
        ""
    ]

    with open(AGH_OUTPUT_FILE, 'w', encoding='utf-8') as f:
        f.write('\n'.join(header + final_output))

    total_end = time.time()
    print("\n" + "="*60)
    print(f"âœ¨ å¤„ç†å®Œæˆï¼ç”Ÿæˆæ–‡ä»¶: {AGH_OUTPUT_FILE}")
    print(f"â±ï¸ æ€»è¿è¡Œè€—æ—¶: {total_end - total_start:.2f} ç§’")
    print(f"ğŸ“Š æœ€ç»ˆè§„æ¨¡: {final_count} æ¡è§„åˆ™")
    print("="*60)

if __name__ == "__main__":
    main()
